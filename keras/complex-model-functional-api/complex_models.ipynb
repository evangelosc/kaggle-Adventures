{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "complex-models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iwP5WZlXxzn"
      },
      "source": [
        "# Complex Models Using the Keras Functional API\n",
        "# For this notebooks we used the California housing problem and tackle it using three ways.\n",
        "# 1) Wide & Deep Neural Network\n",
        "# 2) Wide & Deep Neural Network with 2 inputs\n",
        "# 3) Wide & Deep Neural Network with 2 inputs and 2 outputs\n",
        "# The focus is on the illustration of the Functional API from Keras, not on achieving any high metric"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6M0l2R2ZIl8"
      },
      "source": [
        "# The Wide & Deep Neural Network is able to connect all or part of the inputs directly to the output layer,\n",
        "# hence the Keras Sequential API cannot be used for it."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht-yGcPAZvTC"
      },
      "source": [
        "# Import our Set up\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpQz1I4fZvyv",
        "outputId": "59224e73-a01c-44df-ee62-b445ed0bd3b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVMBngWIjBPd",
        "outputId": "4fc95dc6-4234-4953-c40d-f9b77b89c5c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Model1 - Wide & Deep Neural Network\n",
        "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.Concatenate()([input_, hidden2])\n",
        "output = keras.layers.Dense(1)(concat)\n",
        "model1 = keras.Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "# Compiling and Evaluating the Model\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.00001, nesterov=False, name='SGD')\n",
        "model1.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n",
        "history = model1.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid))\n",
        "mse_test = model1.evaluate(X_test, y_test)\n",
        "\n",
        "# Pretend these are new instances\n",
        "X_new = X_test[:3]\n",
        "y_pred = model1.predict(X_new)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 5.3080 - val_loss: 5.2671\n",
            "Epoch 2/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 5.0911 - val_loss: 5.0443\n",
            "Epoch 3/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 4.8843 - val_loss: 4.8326\n",
            "Epoch 4/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 4.6867 - val_loss: 4.6318\n",
            "Epoch 5/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 4.4981 - val_loss: 4.4406\n",
            "Epoch 6/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 4.3175 - val_loss: 4.2588\n",
            "Epoch 7/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 4.1449 - val_loss: 4.0858\n",
            "Epoch 8/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 3.9798 - val_loss: 3.9212\n",
            "Epoch 9/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 3.8217 - val_loss: 3.7645\n",
            "Epoch 10/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 3.6704 - val_loss: 3.6152\n",
            "Epoch 11/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 3.5256 - val_loss: 3.4729\n",
            "Epoch 12/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 3.3870 - val_loss: 3.3374\n",
            "Epoch 13/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 3.2546 - val_loss: 3.2085\n",
            "Epoch 14/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 3.1279 - val_loss: 3.0856\n",
            "Epoch 15/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 3.0069 - val_loss: 2.9687\n",
            "Epoch 16/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.8912 - val_loss: 2.8574\n",
            "Epoch 17/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.7807 - val_loss: 2.7515\n",
            "Epoch 18/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.6753 - val_loss: 2.6508\n",
            "Epoch 19/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.5749 - val_loss: 2.5551\n",
            "Epoch 20/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.4793 - val_loss: 2.4642\n",
            "Epoch 21/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.3883 - val_loss: 2.3780\n",
            "Epoch 22/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.3018 - val_loss: 2.2961\n",
            "Epoch 23/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.2195 - val_loss: 2.2184\n",
            "Epoch 24/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.1415 - val_loss: 2.1448\n",
            "Epoch 25/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.0674 - val_loss: 2.0750\n",
            "Epoch 26/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.9973 - val_loss: 2.0089\n",
            "Epoch 27/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.9308 - val_loss: 1.9463\n",
            "Epoch 28/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.8679 - val_loss: 1.8871\n",
            "Epoch 29/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.8084 - val_loss: 1.8311\n",
            "Epoch 30/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.7522 - val_loss: 1.7781\n",
            "Epoch 31/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.6992 - val_loss: 1.7280\n",
            "Epoch 32/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.6491 - val_loss: 1.6807\n",
            "Epoch 33/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.6019 - val_loss: 1.6359\n",
            "Epoch 34/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.5573 - val_loss: 1.5936\n",
            "Epoch 35/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.5153 - val_loss: 1.5537\n",
            "Epoch 36/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.4758 - val_loss: 1.5159\n",
            "Epoch 37/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.4385 - val_loss: 1.4802\n",
            "Epoch 38/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.4034 - val_loss: 1.4464\n",
            "Epoch 39/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.3703 - val_loss: 1.4145\n",
            "Epoch 40/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.3393 - val_loss: 1.3844\n",
            "Epoch 41/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.3100 - val_loss: 1.3559\n",
            "Epoch 42/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.2825 - val_loss: 1.3289\n",
            "Epoch 43/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.2566 - val_loss: 1.3034\n",
            "Epoch 44/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.2322 - val_loss: 1.2793\n",
            "Epoch 45/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.2092 - val_loss: 1.2564\n",
            "Epoch 46/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.1876 - val_loss: 1.2348\n",
            "Epoch 47/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.1673 - val_loss: 1.2143\n",
            "Epoch 48/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.1482 - val_loss: 1.1950\n",
            "Epoch 49/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.1302 - val_loss: 1.1766\n",
            "Epoch 50/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.1132 - val_loss: 1.1592\n",
            "Epoch 51/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.0973 - val_loss: 1.1428\n",
            "Epoch 52/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.0822 - val_loss: 1.1272\n",
            "Epoch 53/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.0680 - val_loss: 1.1127\n",
            "Epoch 54/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.0546 - val_loss: 1.0989\n",
            "Epoch 55/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.0420 - val_loss: 1.0858\n",
            "Epoch 56/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.0301 - val_loss: 1.0733\n",
            "Epoch 57/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.0187 - val_loss: 1.0614\n",
            "Epoch 58/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.0080 - val_loss: 1.0500\n",
            "Epoch 59/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.9979 - val_loss: 1.0392\n",
            "Epoch 60/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.9883 - val_loss: 1.0288\n",
            "Epoch 61/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.9791 - val_loss: 1.0189\n",
            "Epoch 62/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.9704 - val_loss: 1.0094\n",
            "Epoch 63/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.9622 - val_loss: 1.0004\n",
            "Epoch 64/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.9543 - val_loss: 0.9917\n",
            "Epoch 65/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.9469 - val_loss: 0.9834\n",
            "Epoch 66/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.9398 - val_loss: 0.9754\n",
            "Epoch 67/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.9330 - val_loss: 0.9678\n",
            "Epoch 68/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.9266 - val_loss: 0.9605\n",
            "Epoch 69/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.9204 - val_loss: 0.9535\n",
            "Epoch 70/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.9145 - val_loss: 0.9467\n",
            "Epoch 71/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.9088 - val_loss: 0.9403\n",
            "Epoch 72/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.9034 - val_loss: 0.9340\n",
            "Epoch 73/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8982 - val_loss: 0.9281\n",
            "Epoch 74/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8933 - val_loss: 0.9223\n",
            "Epoch 75/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8885 - val_loss: 0.9168\n",
            "Epoch 76/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8839 - val_loss: 0.9114\n",
            "Epoch 77/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8795 - val_loss: 0.9063\n",
            "Epoch 78/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8753 - val_loss: 0.9013\n",
            "Epoch 79/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8712 - val_loss: 0.8965\n",
            "Epoch 80/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8673 - val_loss: 0.8919\n",
            "Epoch 81/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8635 - val_loss: 0.8875\n",
            "Epoch 82/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8599 - val_loss: 0.8831\n",
            "Epoch 83/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8564 - val_loss: 0.8790\n",
            "Epoch 84/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8530 - val_loss: 0.8749\n",
            "Epoch 85/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8497 - val_loss: 0.8710\n",
            "Epoch 86/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8465 - val_loss: 0.8673\n",
            "Epoch 87/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8435 - val_loss: 0.8636\n",
            "Epoch 88/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8405 - val_loss: 0.8601\n",
            "Epoch 89/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8376 - val_loss: 0.8566\n",
            "Epoch 90/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8348 - val_loss: 0.8533\n",
            "Epoch 91/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8321 - val_loss: 0.8501\n",
            "Epoch 92/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8295 - val_loss: 0.8470\n",
            "Epoch 93/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8269 - val_loss: 0.8439\n",
            "Epoch 94/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8244 - val_loss: 0.8410\n",
            "Epoch 95/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8220 - val_loss: 0.8381\n",
            "Epoch 96/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8197 - val_loss: 0.8353\n",
            "Epoch 97/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8174 - val_loss: 0.8326\n",
            "Epoch 98/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8151 - val_loss: 0.8299\n",
            "Epoch 99/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8130 - val_loss: 0.8274\n",
            "Epoch 100/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8108 - val_loss: 0.8249\n",
            "162/162 [==============================] - 0s 758us/step - loss: 0.8364\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f92d1b6e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S36dwJ8EkA5N",
        "outputId": "a890355f-a3db-43ca-f6a0-5f5a6442a604",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(y_pred)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.3133554]\n",
            " [2.103262 ]\n",
            " [1.9170855]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO4CIZBuk1_z"
      },
      "source": [
        "# Model2 - Wide & Deep Neural Network with 2 inputs\n",
        "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
        "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.concatenate([input_A, hidden2])\n",
        "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
        "model2 = keras.Model(inputs=[input_A, input_B], outputs=[output])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoFLE2rzl1_P",
        "outputId": "8df46163-bafb-44ae-822f-e02be7c359b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We have to pass a dictionary mapping the input names to the input values\n",
        "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
        "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
        "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
        "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
        "\n",
        "train_dict = {\"wide_input\": X_train_A, \"deep_input\": X_train_B}\n",
        "valid_dict = {\"wide_input\": X_valid_A, \"deep_input\": X_valid_B}\n",
        "test_dict = {\"wide_input\": X_test_A, \"deep_input\": X_test_B}\n",
        "new_dict = {\"wide_input\": X_new_A, \"deep_input\": X_new_B}\n",
        "\n",
        "model2.compile(loss=\"mse\", optimizer=optimizer)\n",
        "history = model2.fit(train_dict, y_train, epochs=20, validation_data=(valid_dict, y_valid))\n",
        "mse_test = model2.evaluate(test_dict, y_test)\n",
        "y_pred = model2.predict(new_dict)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 3.6393 - val_loss: 3.5964\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 3.5496 - val_loss: 3.5126\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 3.4623 - val_loss: 3.4312\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 3.3773 - val_loss: 3.3523\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 3.2946 - val_loss: 3.2757\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 3.2140 - val_loss: 3.2013\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 3.1355 - val_loss: 3.1292\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 3.0591 - val_loss: 3.0592\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.9847 - val_loss: 2.9912\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.9122 - val_loss: 2.9252\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.8416 - val_loss: 2.8612\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.7728 - val_loss: 2.7991\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.7058 - val_loss: 2.7388\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.6406 - val_loss: 2.6803\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.5771 - val_loss: 2.6236\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.5154 - val_loss: 2.5686\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.4552 - val_loss: 2.5153\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.3967 - val_loss: 2.4636\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.3398 - val_loss: 2.4136\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.2845 - val_loss: 2.3651\n",
            "162/162 [==============================] - 0s 908us/step - loss: 2.2870\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f92bef372f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AshxNk-bnZH3",
        "outputId": "6a85d8f0-4b02-49ac-f328-23a984560abf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(y_pred)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.90867376]\n",
            " [0.64235437]\n",
            " [1.0258842 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64IAM1-inrfd"
      },
      "source": [
        "# Model2 - Wide & Deep Neural Network with 2 inputs and 2 outputs\n",
        "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
        "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.concatenate([input_A, hidden2])\n",
        "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
        "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
        "model3 = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBawEPDOqUPs"
      },
      "source": [
        "# Each output needs its own loss function. Therefore, when we compile the model, we should pass a list of losses.\n",
        "# We care much more about the main output than about the auxiliary output (most of the cases it is used for regularization), \n",
        "# so we wanto to give the main output's loss a much grater weight.\n",
        "# The different losses, weight factors, y_valid, y_test and y_train are in a form of a dictionary as the inputs earlier.\n",
        "# We use the data preprocessing we did for the model2."
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW60iBO_q09k"
      },
      "source": [
        "# Dictionaries for losses, loss_weights, y_valid, y_test and y_train\n",
        "loss_dict = {\"main_output\": \"mse\", \"aux_output\": \"mse\"}\n",
        "y_train_dict = {\"main_output\": y_train, \"aux_output\": y_train}\n",
        "weights_dict = {\"main_output\": 0.9, \"aux_output\": 0.1}\n",
        "y_valid_dict = {\"main_output\": y_valid, \"aux_output\": y_valid}\n",
        "y_test_dict = {\"main_output\": y_test, \"aux_output\": y_test}"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MwfjCCgreTA",
        "outputId": "6fd3b0b6-e2f8-4181-c18d-86660af7db14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Compiling and Evaluating the Model\n",
        "model3.compile(loss=loss_dict, loss_weights=weights_dict, optimizer=optimizer)\n",
        "history = model3.fit(train_dict, y_train_dict, epochs=20, validation_data=(valid_dict, y_valid_dict))\n",
        "total_loss, main_loss, aux_loss = model3.evaluate(test_dict, y_test_dict)\n",
        "y_pred_main, y_pred_aux = model3.predict(new_dict)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 3.1100 - main_output_loss: 2.9042 - aux_output_loss: 4.9622 - val_loss: 3.5456 - val_main_output_loss: 3.3834 - val_aux_output_loss: 5.0053\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 3.0561 - main_output_loss: 2.8455 - aux_output_loss: 4.9521 - val_loss: 3.4973 - val_main_output_loss: 3.3306 - val_aux_output_loss: 4.9971\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 3.0038 - main_output_loss: 2.7885 - aux_output_loss: 4.9417 - val_loss: 3.4498 - val_main_output_loss: 3.2789 - val_aux_output_loss: 4.9885\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 2.9530 - main_output_loss: 2.7333 - aux_output_loss: 4.9305 - val_loss: 3.4033 - val_main_output_loss: 3.2282 - val_aux_output_loss: 4.9795\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 2.9036 - main_output_loss: 2.6796 - aux_output_loss: 4.9191 - val_loss: 3.3576 - val_main_output_loss: 3.1785 - val_aux_output_loss: 4.9702\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 2.8556 - main_output_loss: 2.6277 - aux_output_loss: 4.9068 - val_loss: 3.3130 - val_main_output_loss: 3.1299 - val_aux_output_loss: 4.9606\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 2.8090 - main_output_loss: 2.5772 - aux_output_loss: 4.8946 - val_loss: 3.2691 - val_main_output_loss: 3.0822 - val_aux_output_loss: 4.9506\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 2.7637 - main_output_loss: 2.5283 - aux_output_loss: 4.8821 - val_loss: 3.2259 - val_main_output_loss: 3.0354 - val_aux_output_loss: 4.9403\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 2.7196 - main_output_loss: 2.4809 - aux_output_loss: 4.8686 - val_loss: 3.1836 - val_main_output_loss: 2.9896 - val_aux_output_loss: 4.9298\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 2.6769 - main_output_loss: 2.4348 - aux_output_loss: 4.8551 - val_loss: 3.1422 - val_main_output_loss: 2.9447 - val_aux_output_loss: 4.9190\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 2.6353 - main_output_loss: 2.3902 - aux_output_loss: 4.8414 - val_loss: 3.1014 - val_main_output_loss: 2.9007 - val_aux_output_loss: 4.9080\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 2.5949 - main_output_loss: 2.3469 - aux_output_loss: 4.8275 - val_loss: 3.0613 - val_main_output_loss: 2.8573 - val_aux_output_loss: 4.8967\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 2.5557 - main_output_loss: 2.3049 - aux_output_loss: 4.8131 - val_loss: 3.0219 - val_main_output_loss: 2.8148 - val_aux_output_loss: 4.8852\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 2.5176 - main_output_loss: 2.2642 - aux_output_loss: 4.7983 - val_loss: 2.9832 - val_main_output_loss: 2.7732 - val_aux_output_loss: 4.8736\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 2.4806 - main_output_loss: 2.2247 - aux_output_loss: 4.7836 - val_loss: 2.9451 - val_main_output_loss: 2.7322 - val_aux_output_loss: 4.8617\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 2.4446 - main_output_loss: 2.1864 - aux_output_loss: 4.7683 - val_loss: 2.9078 - val_main_output_loss: 2.6920 - val_aux_output_loss: 4.8497\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 2.4096 - main_output_loss: 2.1493 - aux_output_loss: 4.7529 - val_loss: 2.8710 - val_main_output_loss: 2.6525 - val_aux_output_loss: 4.8375\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 2.3756 - main_output_loss: 2.1132 - aux_output_loss: 4.7374 - val_loss: 2.8349 - val_main_output_loss: 2.6137 - val_aux_output_loss: 4.8252\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 2.3426 - main_output_loss: 2.0783 - aux_output_loss: 4.7215 - val_loss: 2.7994 - val_main_output_loss: 2.5757 - val_aux_output_loss: 4.8126\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 2.3105 - main_output_loss: 2.0444 - aux_output_loss: 4.7059 - val_loss: 2.7644 - val_main_output_loss: 2.5383 - val_aux_output_loss: 4.8000\n",
            "162/162 [==============================] - 0s 2ms/step - loss: 2.2899 - main_output_loss: 2.0117 - aux_output_loss: 4.7944\n",
            "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f92b89116a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA9AxU2Xs6Yf",
        "outputId": "dd320e79-e864-4854-ab84-33c718a17552",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(y_pred_main)\n",
        "print(y_pred_aux)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.305287  ]\n",
            " [0.74481755]\n",
            " [1.5974648 ]]\n",
            "[[1.0650762 ]\n",
            " [0.1066421 ]\n",
            " [0.15939891]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj9g3pMxtdDn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}